<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Clawdbot爆火？揭开它的面纱！ | mengzi&#39;s blog</title>
  <meta name="keywords" content=" AI Agent ">
  <meta name="description" content="Clawdbot爆火？揭开它的面纱！ | mengzi&#39;s blog">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="个人简介mengzi，一个无趣、严重精神内耗、闲得发慌还电子阳痿之人 喜欢看电影、听音乐、玩玩轻松愉悦的游戏和折腾电子设备 目前来说最大的遗憾是因为读书没能坚持学小提琴，希望有一天能重新把小提琴拾起 经典语录一阶段：买买买！不买还是人？ 二阶段：游戏我花钱买了还要我花时间玩？ 最近期待8月3号博德之门3正式版发布 电子阳痿？博启！ 联系方式 邮箱： mengziforever@qq.com">
<meta property="og:type" content="website">
<meta property="og:title" content="关于&amp;留言板">
<meta property="og:url" content="http://example.com/about/index.html">
<meta property="og:site_name" content="mengzi&#39;s blog">
<meta property="og:description" content="个人简介mengzi，一个无趣、严重精神内耗、闲得发慌还电子阳痿之人 喜欢看电影、听音乐、玩玩轻松愉悦的游戏和折腾电子设备 目前来说最大的遗憾是因为读书没能坚持学小提琴，希望有一天能重新把小提琴拾起 经典语录一阶段：买买买！不买还是人？ 二阶段：游戏我花钱买了还要我花时间玩？ 最近期待8月3号博德之门3正式版发布 电子阳痿？博启！ 联系方式 邮箱： mengziforever@qq.com">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon6.png">
<meta property="article:published_time" content="2023-07-23T08:43:45.000Z">
<meta property="article:modified_time" content="2023-08-08T14:30:20.000Z">
<meta property="article:author" content="mengzi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gsp0.baidu.com/5aAHeD3nKhI2p27j8IqW0jdnxx1xbK/tb/editor/images/client/image_emoticon6.png">


<link rel="icon" href="/img/favicon.png">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.1.0" ></script>

<script src="/js/custom-iconfont.js?v=1.1.0" ></script>
<meta name="generator" content="Hexo 6.3.0"></head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="false" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/"
   class="avatar_target">
    <img class="avatar"
         src="/img/avatar.png"/>
</a>
<div class="author">
    <span>mengzi</span>
</div>

<div class="icon">
    
        
    
        
            <a title="github"
               href="https://github.com/rosemengzi"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-github"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="gitee"
               href="https://gitee.com/mengzi_777"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-gitee"></use>
                    </svg>
                
            </a>
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            <a title="email"
               href="mailto:mailto:mengziforever@qq.com"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-email"></use>
                    </svg>
                
            </a>
        
    
        
    
        
    
        
            <a title="neteasemusic"
               href="https://music.163.com/#/user/home?id=379625478"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-neteasemusic"></use>
                    </svg>
                
            </a>
        
    
        
            <a title="spotify"
               href="https://open.spotify.com/playlist/3L0cIrnkEYWf4fvZekAzYT"
               target="_blank">
                
                    <svg class="iconfont-svg" aria-hidden="true">
                        <use xlink:href="#icon-spotify"></use>
                    </svg>
                
            </a>
        
    
</div>





<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(28)</small>
            
        </div>
    </li>
    
        
            
                
    <li>
        <div data-rel="电影">
            <i class="fold iconfont icon-right"></i>
            电影
            <small>(14)</small>
        </div>
        
            <ul class="sub hide">
                
                    
    <li>
        <div data-rel="电影&lt;---&gt;好片推荐">
            
            好片推荐
            <small>(3)</small>
        </div>
        
    </li>

                
                    
    <li>
        <div data-rel="电影&lt;---&gt;看世界">
            
            看世界
            <small>(1)</small>
        </div>
        
    </li>

                
                    
    <li>
        <div data-rel="电影&lt;---&gt;年度电影总结">
            
            年度电影总结
            <small>(2)</small>
        </div>
        
    </li>

                
                    
    <li>
        <div data-rel="电影&lt;---&gt;月度电影小结">
            
            月度电影小结
            <small>(5)</small>
        </div>
        
    </li>

                
            </ul>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="好物开箱">
            
            好物开箱
            <small>(1)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="杂谈">
            
            杂谈
            <small>(9)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="NAS">
            
            NAS
            <small>(1)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="分享">
            <i class="fold iconfont icon-right"></i>
            分享
            <small>(2)</small>
        </div>
        
            <ul class="sub hide">
                
                    
    <li>
        <div data-rel="分享&lt;---&gt;好文推荐">
            
            好文推荐
            <small>(2)</small>
        </div>
        
    </li>

                
            </ul>
        
    </li>

            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
            
            
    </div>
    <div>
        
            <a class="about  hasFriend  site_url"
               
               href="/about">关于</a>
        
        <a style="width: 50%"
                
                                           class="friends">外链</a>
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="28">
<input type="hidden" id="yelog_site_word_count" value="62.7k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        全部链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="https://mengzi.icu/">mengzi主页</a></li>
            
            <li><a target="_blank" href="https://996.icu/#/zh_CN/">996.ICU</a></li>
            
            <li><a target="_blank" href="https://yelog.org/">叶落阁</a></li>
            
            <li><a target="_blank" href="https://www.huxiu.com/member/2584088/">枪稿1</a></li>
            
            <li><a target="_blank" href="https://www.toutiao.com/c/user/token/MS4wLjABAAAAKAJekOxFh9D8VXDO2n--UmgyjaSvjfeKLTygyhkVhp-XRiMZ4CH4PRm4Fzf-KE02">枪稿2</a></li>
            
            <li><a target="_blank" href="https://www.notion.so/404-b2128e762bf3429494773615423c0e9b">枪稿404文章</a></li>
            
            <li><a target="_blank" href="https://www.quchao.net/">Mark&#39;s Blog</a></li>
            
            <li><a target="_blank" href="https://www.imaegoo.com/2020/aes-key-generator/">AES 密钥在线生成器</a></li>
            
            <li><a target="_blank" href="https://powersee.github.io/">powersee&#39;s Blog</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>独立电影</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>歌舞片</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>好片推荐</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>好物开箱</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>看世界</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>枪稿</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>威联通</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>演唱会电影</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>影视杂谈</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>月度电影小结</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>运营</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>转载</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>AI Agent</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>AliExpress</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>private</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>Taylor Swift</a>
            </li>
        
    </div>

</div>

    
    <nav id="title-list-nav">
        
        
        <a  class="全部文章 "
           href="/2026/02/18/Clawdbot/"
           data-tag="AI Agent"
           data-author="" >
            <span class="post-title" title="Clawdbot爆火？揭开它的面纱！">Clawdbot爆火？揭开它的面纱！</span>
            <span class="post-date" title="2026-02-18 18:16:18">2026/02/18</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2026/01/25/%E6%9D%82%E8%B0%88%E7%94%B5%E5%95%86%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AFAI/"
           data-tag="AliExpress,运营"
           data-author="" >
            <span class="post-title" title="从电商实战看搜索：为什么你改了关键词，广告却没起色">从电商实战看搜索：为什么你改了关键词，广告却没起色</span>
            <span class="post-date" title="2026-01-25 22:58:11">2026/01/25</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2026/01/23/%E6%9D%82%E8%B0%88%E7%94%B5%E5%95%86%E6%90%9C%E7%B4%A2%E6%8A%80%E6%9C%AF/"
           data-tag="AliExpress,运营"
           data-author="" >
            <span class="post-title" title="2026.1.23杂谈电商搜索技术">2026.1.23杂谈电商搜索技术</span>
            <span class="post-date" title="2026-01-23 20:58:19">2026/01/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2026/01/06/%E6%BC%AB%E8%B0%88%E6%B8%B8%E6%88%8F/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="游戏——“第九艺术”">游戏——“第九艺术”</span>
            <span class="post-date" title="2026-01-06 21:46:47">2026/01/06</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2025/08/10/%E4%B8%8A%E5%BD%93/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="《上当》">《上当》</span>
            <span class="post-date" title="2025-08-10 21:10:56">2025/08/10</span>
        </a>
        
        
        <a  class="全部文章 电影 年度电影总结 "
           href="/2024/12/30/%E8%B1%86%E7%93%A32024%E5%B9%B4%E5%BA%A6%E6%A6%9C%E5%8D%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="豆瓣2024年度榜单">豆瓣2024年度榜单</span>
            <span class="post-date" title="2024-12-30 16:11:41">2024/12/30</span>
        </a>
        
        
        <a  class="全部文章 分享 好文推荐 "
           href="/2024/03/25/%E6%9E%AA%E7%A8%BF%E8%B0%88%E7%9B%97%E7%89%88/"
           data-tag="转载,枪稿"
           data-author="" >
            <span class="post-title" title="看盗版，你不觉得羞耻吗？ | 开寅">看盗版，你不觉得羞耻吗？ | 开寅</span>
            <span class="post-date" title="2024-03-25 15:16:39">2024/03/25</span>
        </a>
        
        
        <a  class="全部文章 分享 好文推荐 "
           href="/2024/03/25/%E6%9E%AA%E7%A8%BF%E8%B0%88%E8%83%96%E9%B8%9F/"
           data-tag="转载,枪稿"
           data-author="" >
            <span class="post-title" title="我们为什么哀悼胖鸟的倒下？ | 杨时旸专栏">我们为什么哀悼胖鸟的倒下？ | 杨时旸专栏</span>
            <span class="post-date" title="2024-03-25 11:40:28">2024/03/25</span>
        </a>
        
        
        <a  class="全部文章 电影 好片推荐 "
           href="/2024/03/20/%E5%91%A8%E5%A4%84%E9%99%A4%E4%B8%89%E5%AE%B3/"
           data-tag="好片推荐"
           data-author="" >
            <span class="post-title" title="周处除三害观后感">周处除三害观后感</span>
            <span class="post-date" title="2024-03-20 15:45:49">2024/03/20</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/02/22/2-22%E6%9D%82%E8%B0%88/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="2.22杂谈“史上最热闹”春节档">2.22杂谈“史上最热闹”春节档</span>
            <span class="post-date" title="2024-02-22 09:48:56">2024/02/22</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/02/17/2-17%E6%9D%82%E8%B0%88/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="2.17杂谈屏摄">2.17杂谈屏摄</span>
            <span class="post-date" title="2024-02-17 23:22:04">2024/02/17</span>
        </a>
        
        
        <a  class="全部文章 电影 看世界 "
           href="/2024/02/16/%E9%87%8C%E7%BA%A6%E7%83%AD%E5%86%85%E5%8D%A2/"
           data-tag="看世界"
           data-author="" >
            <span class="post-title" title="里约热内卢">里约热内卢</span>
            <span class="post-date" title="2024-02-16 06:16:58">2024/02/16</span>
        </a>
        
        
        <a  class="全部文章 电影 月度电影小结 "
           href="/2024/02/16/11%E6%9C%88%E7%94%B5%E5%BD%B1%E5%B0%8F%E7%BB%93/"
           data-tag="月度电影小结"
           data-author="" >
            <span class="post-title" title="2023年11月电影个人小结">2023年11月电影个人小结</span>
            <span class="post-date" title="2024-02-16 03:42:15">2024/02/16</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/02/13/%E7%BA%AA%E5%AE%9E%E7%9F%AD%E7%89%87%E5%A4%87%E4%BB%BD/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="纪实短片备份">纪实短片备份</span>
            <span class="post-date" title="2024-02-13 23:44:49">2024/02/13</span>
        </a>
        
        
        <a  class="全部文章 电影 月度电影小结 "
           href="/2024/01/22/10%E6%9C%88%E7%94%B5%E5%BD%B1%E5%B0%8F%E7%BB%93/"
           data-tag="月度电影小结"
           data-author="" >
            <span class="post-title" title="2023年10月电影个人小结">2023年10月电影个人小结</span>
            <span class="post-date" title="2024-01-22 14:00:32">2024/01/22</span>
        </a>
        
        
        <a  class="全部文章 电影 "
           href="/2024/01/06/%E5%85%B3%E4%BA%8E%E6%AD%8C%E8%88%9E%E7%89%87/"
           data-tag="歌舞片"
           data-author="" >
            <span class="post-title" title="唱唱歌跳跳舞">唱唱歌跳跳舞</span>
            <span class="post-date" title="2024-01-06 14:37:48">2024/01/06</span>
        </a>
        
        
        <a  class="全部文章 电影 "
           href="/2024/01/05/Taylor-The-Eras-Tour2/"
           data-tag="Taylor Swift,演唱会电影"
           data-author="" >
            <span class="post-title" title="泰勒·斯威夫特：时代巡回演唱会(Part2)">泰勒·斯威夫特：时代巡回演唱会(Part2)</span>
            <span class="post-date" title="2024-01-05 21:37:35">2024/01/05</span>
        </a>
        
        
        <a  class="全部文章 电影 好片推荐 "
           href="/2024/01/04/Taylor-The-Eras-Tour1/"
           data-tag="好片推荐,Taylor Swift"
           data-author="" >
            <span class="post-title" title="泰勒·斯威夫特：时代巡回演唱会(Part1)">泰勒·斯威夫特：时代巡回演唱会(Part1)</span>
            <span class="post-date" title="2024-01-04 16:03:00">2024/01/04</span>
        </a>
        
        
        <a  class="全部文章 电影 年度电影总结 "
           href="/2023/12/26/%E8%B1%86%E7%93%A32023%E5%B9%B4%E5%BA%A6%E6%A6%9C%E5%8D%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="豆瓣2023年度榜单">豆瓣2023年度榜单</span>
            <span class="post-date" title="2023-12-26 16:35:46">2023/12/26</span>
        </a>
        
        
        <a  class="全部文章 电影 "
           href="/2023/10/03/%E6%BC%AB%E8%B0%88%E7%A0%B4%E9%98%B2%E7%94%B5%E5%BD%B1/"
           data-tag="影视杂谈"
           data-author="" >
            <span class="post-title" title="看个电影，你就破防啦？">看个电影，你就破防啦？</span>
            <span class="post-date" title="2023-10-03 00:11:32">2023/10/03</span>
        </a>
        
        
        <a  class="全部文章 电影 月度电影小结 "
           href="/2023/10/02/9%E6%9C%88%E7%94%B5%E5%BD%B1%E5%B0%8F%E7%BB%93/"
           data-tag="月度电影小结"
           data-author="" >
            <span class="post-title" title="2023年9月电影个人小结">2023年9月电影个人小结</span>
            <span class="post-date" title="2023-10-02 16:39:32">2023/10/02</span>
        </a>
        
        
        <a  class="全部文章 电影 好片推荐 "
           href="/2023/09/21/A-Ghost-Story%E6%8E%A8%E8%8D%90/"
           data-tag="好片推荐,独立电影"
           data-author="" >
            <span class="post-title" title="A Ghost Story推荐">A Ghost Story推荐</span>
            <span class="post-date" title="2023-09-21 18:56:00">2023/09/21</span>
        </a>
        
        
        <a  class="全部文章 NAS "
           href="/2023/09/06/QNAP%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/"
           data-tag="威联通"
           data-author="" >
            <span class="post-title" title="QNAP避坑指南">QNAP避坑指南</span>
            <span class="post-date" title="2023-09-06 20:55:54">2023/09/06</span>
        </a>
        
        
        <a  class="全部文章 电影 月度电影小结 "
           href="/2023/09/04/8%E6%9C%88%E7%94%B5%E5%BD%B1%E5%B0%8F%E7%BB%93/"
           data-tag="月度电影小结"
           data-author="" >
            <span class="post-title" title="2023年8月电影个人小结">2023年8月电影个人小结</span>
            <span class="post-date" title="2023-09-04 21:52:19">2023/09/04</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2023/08/24/8-24%E6%9D%82%E8%B0%88/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="8.24杂谈开屏广告">8.24杂谈开屏广告</span>
            <span class="post-date" title="2023-08-24 21:57:57">2023/08/24</span>
        </a>
        
        
        <a  class="全部文章 好物开箱 "
           href="/2023/08/08/%E5%A8%81%E8%81%94%E9%80%9ANAS%E5%BC%80%E7%AE%B1/"
           data-tag="威联通,好物开箱"
           data-author="" >
            <span class="post-title" title="威联通NAS开箱">威联通NAS开箱</span>
            <span class="post-date" title="2023-08-08 22:10:32">2023/08/08</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2023/07/31/7-31%E6%9D%82%E8%B0%88/"
           data-tag="private"
           data-author="" >
            <span class="post-title" title="7.31杂谈">7.31杂谈</span>
            <span class="post-date" title="2023-07-31 23:25:00">2023/07/31</span>
        </a>
        
        
        <a  class="全部文章 电影 月度电影小结 "
           href="/2023/07/01/6%E6%9C%88%E7%94%B5%E5%BD%B1%E5%B0%8F%E7%BB%93/"
           data-tag="月度电影小结"
           data-author="" >
            <span class="post-title" title="2023年6月电影个人小结">2023年6月电影个人小结</span>
            <span class="post-date" title="2023-07-01 01:28:05">2023/07/01</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-Clawdbot" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">Clawdbot爆火？揭开它的面纱！</h1>
    
    <div class="article-meta">
        
        
        
        
        <span class="tag">
            <i class="iconfont icon-tag"></i>
            
            <a class="color4">AI Agent</a>
            
        </span>
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title='最后更新: 2026-02-19 18:08:41'>2026-02-18 18:16</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:6.1k</span>
        
        
        <span id="busuanzi_container_page_pv">
            阅读 :<span id="busuanzi_value_page_pv">
                <span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </span>
        </span>
        
        
        <span class="top-comment" title="跳转至评论区">
            <a href="#comments">
                评论:<span class="count-comment">
                    <span class="spinner">
                      <div class="cube1"></div>
                      <div class="cube2"></div>
                    </span>
                </span>
            </a>
        </span>
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM"><span class="toc-text">LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2%EF%BC%9A%E4%BB%8E%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B%E5%88%B0%E7%94%9F%E6%88%90%E5%BC%8FAI"><span class="toc-text">发展历史：从统计模型到生成式AI</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%8A%80%E6%9C%AF%E8%B5%B7%E6%BA%90%EF%BC%881950s-1980s%EF%BC%89%EF%BC%9A%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">1. 技术起源（1950s-1980s）：统计语言模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%9B%8F%E5%BD%A2%E9%98%B6%E6%AE%B5%EF%BC%881990s-2010s%EF%BC%89%EF%BC%9A%E7%A5%9E%E7%BB%8F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">2. 雏形阶段（1990s-2010s）：神经语言模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-GPT%E6%A8%A1%E5%9E%8B%E9%97%AE%E4%B8%96%EF%BC%882018-2020%EF%BC%89%EF%BC%9A%E7%94%9F%E6%88%90%E5%BC%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">3. GPT模型问世（2018-2020）：生成式预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%BF%9B%E9%98%B6%E7%AA%81%E7%A0%B4%EF%BC%882021%E8%87%B3%E4%BB%8A%EF%BC%89%EF%BC%9A%E5%AF%B9%E9%BD%90%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81"><span class="toc-text">4. 进阶突破（2021至今）：对齐与多模态</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Memory"><span class="toc-text">Memory</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%9C%80%E5%9F%BA%E7%A1%80%E5%B1%82%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AE%B0%E5%BF%86%EF%BC%88Context-Memory%EF%BC%89"><span class="toc-text">一、最基础层：上下文记忆（Context Memory）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%BC%9A%E8%AF%9D%E7%BA%A7%E8%AE%B0%E5%BF%86%EF%BC%88Session-Memory%EF%BC%89"><span class="toc-text">二、会话级记忆（Session Memory）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E9%95%BF%E6%9C%9F%E8%AE%B0%E5%BF%86%EF%BC%88Persistent-Memory%EF%BC%89"><span class="toc-text">三、长期记忆（Persistent Memory）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%90%91%E9%87%8F%E8%AE%B0%E5%BF%86%EF%BC%88Vector-Memory%EF%BC%89"><span class="toc-text">四、向量记忆（Vector Memory）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E5%86%85%E9%83%A8%E2%80%9C%E8%AE%B0%E5%BF%86%E2%80%9D%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">五、模型内部“记忆”是什么？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG"><span class="toc-text">RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%9E%84%E5%BB%BA%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93"><span class="toc-text">索引阶段：构建外部知识库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A3%80%E7%B4%A2%E9%98%B6%E6%AE%B5%EF%BC%9A%E8%8E%B7%E5%8F%96%E7%9B%B8%E5%85%B3%E8%AF%81%E6%8D%AE"><span class="toc-text">检索阶段：获取相关证据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E5%BC%BA%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%9E%84%E9%80%A0%E5%A2%9E%E5%BC%BA%E6%8F%90%E7%A4%BA"><span class="toc-text">增强阶段：构造增强提示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E9%98%B6%E6%AE%B5%EF%BC%9A%E8%BE%93%E5%87%BA%E5%87%86%E7%A1%AE%E7%AD%94%E6%A1%88"><span class="toc-text">生成阶段：输出准确答案</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%8F%E5%B0%91%E5%B9%BB%E8%A7%89%EF%BC%8C%E6%8F%90%E5%8D%87%E5%87%86%E7%A1%AE%E6%80%A7"><span class="toc-text">减少幻觉，提升准确性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E6%9B%B4%E6%96%B0%E7%9F%A5%E8%AF%86%EF%BC%8C%E7%AA%81%E7%A0%B4%E2%80%9C%E7%9F%A5%E8%AF%86%E5%A4%A9%E8%8A%B1%E6%9D%BF%E2%80%9D"><span class="toc-text">实时更新知识，突破“知识天花板”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A2%9E%E5%BC%BA%E5%8F%AF%E8%BF%BD%E6%BA%AF%E6%80%A7%EF%BC%8C%E6%8F%90%E5%8D%87%E7%94%A8%E6%88%B7%E4%BF%A1%E4%BB%BB"><span class="toc-text">增强可追溯性，提升用户信任</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%99%8D%E4%BD%8E%E6%88%90%E6%9C%AC%EF%BC%8C%E6%8F%90%E9%AB%98%E6%95%88%E7%8E%87"><span class="toc-text">降低成本，提高效率</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MCP"><span class="toc-text">MCP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Skills"><span class="toc-text">Skills</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AI-Agent"><span class="toc-text">AI Agent</span></a></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-3 i,
    .toc-level-3 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>　　2026年初，Clawdbot迅速出圈，引发了广泛的社会和市场反响，其实我很早就想研究部署这么个玩意了，但因为上班+懒狗双重Debuff，导致一直到春节假期我才开始了解它，简单介绍一下Clawdbot，准确来说Clawdbot现在的名字是OpenClaw。</p>
<p>OpenClaw 的发展历程充满戏剧性，其名称变更直接引发了一场金融事件，</p>
<p>起源‌：项目最初由奥地利开发者 Peter Steinberger 于 2025 年底开源，名为 ‌Clawdbot‌。‌‌</p>
<p>更名风波‌：2026 年初，因名称与 AI 公司 Anthropic 的产品“Claude”相似，面临商标侵权诉讼。项目先后更名为 ‌Moltbot‌，最终定名为 ‌OpenClaw‌。‌<br>‌“10 秒钟惨案”‌：在旧名称（Clawdbot）的 GitHub 账号和社交媒体句柄被弃用、新名称（OpenClaw）尚未注册的约 10 秒空窗期内，币圈机器人抢注了原账号，并发售了名为 $CLAWD 的代币，市值瞬间冲至 1600 万美元后又“跑路”归零，成为 2026 年初科技圈的重大事件。‌‌</p>
<p>我觉得想要真正的了解Clawdbot是什么，就需要知道一下几个概念，LLM、Memory、RAG、MCP、Skills。</p>
<h2 id="LLM"><a href="#LLM" class="headerlink" title="LLM"></a>LLM</h2><p>　　LLM 是 <strong>Large Language Model</strong> 的缩写，中文叫“大语言模型”。也就是现在的各大推理模型，DeepSeek等等，LLM是如何训练的？1. 预训练（Pretraining）：在海量文本上训练“<strong>预测下一个词</strong>”。2. 微调（Fine-tuning）用人工标注数据让模型更符合人类期望。很多模型还加了RLHF（人类反馈强化学习）和对齐训练（Alignment）。</p>
<h3 id="发展历史：从统计模型到生成式AI"><a href="#发展历史：从统计模型到生成式AI" class="headerlink" title="发展历史：从统计模型到生成式AI"></a>发展历史：从统计模型到生成式AI</h3><p>LLM的发展经历了四个关键阶段：</p>
<h4 id="1-技术起源（1950s-1980s）：统计语言模型"><a href="#1-技术起源（1950s-1980s）：统计语言模型" class="headerlink" title="1. 技术起源（1950s-1980s）：统计语言模型"></a>1. <strong>技术起源（1950s-1980s）：统计语言模型</strong></h4><p>1950s：图灵测试提出，探索机器对人类语言的理解；</p>
<p>1970s：N-gram模型（如二元模型、三元模型）成为主流，通过统计连续N个Token的频率，预测下一个Token（如“我吃”后面接“饭”的概率）；</p>
<p>局限：数据稀疏（如“我吃月球”的概率为0）、无法捕捉长距离依赖（如“猫坐在沙发上，它很舒服”中的“它”指“猫”）。</p>
<h4 id="2-雏形阶段（1990s-2010s）：神经语言模型"><a href="#2-雏形阶段（1990s-2010s）：神经语言模型" class="headerlink" title="2. 雏形阶段（1990s-2010s）：神经语言模型"></a>2. <strong>雏形阶段（1990s-2010s）：神经语言模型</strong></h4><p>1990s：神经网络开始应用于语言建模（如循环神经网络RNN、长短期记忆网络LSTM），解决长距离依赖问题；</p>
<p>2013：Word2Vec模型提出，将单词映射到低维向量空间（如“国王”的向量减去“男人”加上“女人”等于“女王”），捕捉语义关系；</p>
<p>2017：Transformer架构提出，取代RNN&#x2F;LSTM成为LLM的基础，解决了并行计算问题（RNN无法并行处理序列，Transformer可通过自注意力并行计算所有Token）。</p>
<h4 id="3-GPT模型问世（2018-2020）：生成式预训练模型"><a href="#3-GPT模型问世（2018-2020）：生成式预训练模型" class="headerlink" title="3. GPT模型问世（2018-2020）：生成式预训练模型"></a>3. <strong>GPT模型问世（2018-2020）：生成式预训练模型</strong></h4><p>2018：Google发布BERT（双向Transformer），通过MLM任务学习上下文表示，在11项NLP任务中取得 state-of-the-art 结果；</p>
<p>2018：OpenAI发布GPT-1（生成式预训练Transformer），采用单向Transformer（仅关注左侧上下文），用于文本生成；</p>
<p>2019：GPT-2发布，参数规模达15亿，具备零样本学习能力（如无需训练即可完成翻译、摘要）；</p>
<p>2020：GPT-3发布，参数规模达1750亿，开启“大模型时代”，在少样本学习中表现突出（如用10个例子即可学会写代码）。</p>
<h4 id="4-进阶突破（2021至今）：对齐与多模态"><a href="#4-进阶突破（2021至今）：对齐与多模态" class="headerlink" title="4. 进阶突破（2021至今）：对齐与多模态"></a>4. <strong>进阶突破（2021至今）：对齐与多模态</strong></h4><p><strong>2022</strong>：ChatGPT发布，基于GPT-3.5，通过RLHF对齐人类价值观，实现流畅的对话生成，引发全球关注；</p>
<p>2023：GPT-4发布，参数规模达1.8万亿（采用专家混合模型MoE，即16个专家网络，每个专家负责不同任务），支持多模态输入（文本+图像）；</p>
<p>2024：Llama 3、Claude 3等开源模型发布，参数规模达数百亿，性能接近GPT-4，推动LLM的普及；</p>
<p>2025：DeepSeek-R1、文心一言4.0等国内模型发布，强调本地化与合规性（如数据驻留、敏感词过滤），在金融、政务等场景落地。</p>
<p>LLM不是数据库，它不是存答案，而是一个高维概率函数，你问问题时，它不是去“查资料”，而是在概率空间里生成最可能合理的回答。这也是为什么会出现“<strong>幻觉</strong>（Hallucination）”。<strong>它追求概率最大，不是真实性最大</strong>。它本质<strong>是统计机器</strong>，<strong>不是意识体</strong>。</p>
<h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><p>　　Memory是LLM的一套机制，<strong>模型本体其实没有“长期记忆”</strong>。它只是在当前输入里看到什么，就基于那些内容做概率预测。那为什么我们在AI聊天页面里，明显感觉它能“记得”我们之前的对话呢？</p>
<h4 id="一、最基础层：上下文记忆（Context-Memory）"><a href="#一、最基础层：上下文记忆（Context-Memory）" class="headerlink" title="一、最基础层：上下文记忆（Context Memory）"></a>一、最基础层：上下文记忆（Context Memory）</h4><p>这是 LLM 最核心、最原始的“记忆”。模型一次推理时，会看到一整段输入文本（叫 context）。包括：系统提示、历史对话、当前问题，它并没有“存储历史”，而是每次都把历史重新输入进去。<strong>本质是：重新阅读，而不是回忆。</strong>这是一种“瞬时记忆”。限制在哪里？<strong>上下文长度是有限的</strong>（例如 8k、32k、128k tokens）。超过长度，旧内容就会被截断。这就是为什么：对话久了模型会“忘记前面”以及长文档会丢信息，<strong>记忆容量 &#x3D; 上下文窗口大小</strong>。</p>
<h4 id="二、会话级记忆（Session-Memory）"><a href="#二、会话级记忆（Session-Memory）" class="headerlink" title="二、会话级记忆（Session Memory）"></a>二、会话级记忆（Session Memory）</h4><p>这是系统层面的。很多 AI 应用会把对话历史存到数据库里，然后每次请求时把相关部分再拼接进 prompt。模型本体不知道这是“记忆”。它只看到一段更长的文本。这属于：<strong>记忆外包</strong>。</p>
<h4 id="三、长期记忆（Persistent-Memory）"><a href="#三、长期记忆（Persistent-Memory）" class="headerlink" title="三、长期记忆（Persistent Memory）"></a>三、长期记忆（Persistent Memory）</h4><p>更高级的系统会做结构化存储，比如：用户偏好、角色设定、关键事实、过往任务总结。然后在新对话时：<strong>1.</strong> 通过检索找到相关记忆，<strong>2.</strong> 注入到 prompt，<strong>3.</strong> 再生成回答。这就是 <strong>RAG</strong>（<strong>Retrieval-Augmented Generation</strong>，检索增强生成）。这才是接近“长期记忆”的形态。但本质仍然是：<strong>数据库存储 + 检索 + 拼接</strong>，模型本体仍然没有自发记忆能力。</p>
<h4 id="四、向量记忆（Vector-Memory）"><a href="#四、向量记忆（Vector-Memory）" class="headerlink" title="四、向量记忆（Vector Memory）"></a>四、向量记忆（Vector Memory）</h4><p>这是目前最常见的“高级记忆”实现方式。步骤：<strong>1.</strong> 把文本转成<strong>向量</strong>（embedding），<strong>2.</strong> 存入<strong>向量数据库</strong>，<strong>3.</strong> 新问题也<strong>转向量</strong>，<strong>4.</strong> 找最相似的历史内容，<strong>5.</strong> 插入 prompt。优点：可扩展、支持语义检索、不需要精确匹配。这就是为什么很多 AI 系统可以“记住你以前说过的话”。但它不是主动回忆，而是相似度匹配。</p>
<h4 id="五、模型内部“记忆”是什么？"><a href="#五、模型内部“记忆”是什么？" class="headerlink" title="五、模型内部“记忆”是什么？"></a>五、模型内部“记忆”是什么？</h4><p>模型内部确实“记住”了大量知识。但那是<strong>参数记忆</strong>（Parameter Memory）。LLM 的参数是经过海量文本训练后的权重。知识被编码进权重矩阵。这叫：<strong>权重记忆（Weight Memory）</strong>特点：静态、不可编辑、不可针对个人更新。除非重新训练或微调。</p>
<p>知道了Memory，我们也就明白了为什么LLM会“忘记”，因为它没有真正的状态保存机制，每次调用都是一次独立运算，它不像人脑有连续的神经活动，当前的LLM是“无状态生成器”，它像一个超级聪明的计算器，但没有自我时间轴。LLM的Memory本质不是“记住”，而是“<strong>重新看到</strong>”。</p>
<p><strong>记忆，是AI从工具走向真正智能体的关键缺口。</strong></p>
<h2 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h2><p>　　RAG 是 <strong>Retrieval-Augmented Generation</strong>，中文一般叫“检索增强生成”。RAG是一种<strong>结合信息检索与文本生成的人工智能框架</strong>，旨在通过引用外部权威知识库，<strong>解决</strong>大型语言模型（LLM）的“<strong>幻觉</strong>”（编造虚假信息）、“<strong>知识过时</strong>”（无法获取最新信息）、“<strong>不可追溯</strong>”（回答无来源依据）等固有缺陷。RAG的核心理念是<strong>“教会模型查资料”</strong>：在回答用户问题时，系统先从外部知识源（如文档数据库、网页、知识图谱）中检索与问题最相关的信息片段，再将这些“证据”与原始问题一同提交给LLM，指示模型基于证据生成答案。这如同学者撰写论文时先查阅文献再引用，而非仅凭记忆，使回答更具可信度。</p>
<p>RAG的典型流程可概括为<strong>“索引-检索-增强-生成”</strong>四个阶段</p>
<ol>
<li><h4 id="索引阶段：构建外部知识库"><a href="#索引阶段：构建外部知识库" class="headerlink" title="索引阶段：构建外部知识库"></a><strong>索引阶段：构建外部知识库</strong></h4><p><strong>输入</strong>：企业内部文档、产品手册、项目报告、网页信息等非结构化&#x2F;半结构化数据。</p>
<p><strong>处理</strong>：</p>
<ol>
<li><p><strong>分块</strong>：将长文档拆分为适合LLM处理的短片段（如512-1024 tokens），避免信息过载；</p>
</li>
<li><p><strong>清洗</strong>：去除噪声（如广告、重复内容）、统一格式（如PDF转文本）；</p>
</li>
<li><p><strong>向量化</strong>：使用嵌入模型（如Sentence-BERT、OpenAI Embeddings）将文本转换为高维向量（嵌入向量），捕捉语义信息；</p>
</li>
<li><p><strong>存储</strong>：将嵌入向量存入<strong>向量数据库</strong>（如Chroma、Pinecone、Milvus），构建索引以支持快速检索。</p>
</li>
</ol>
</li>
<li><h4 id="检索阶段：获取相关证据"><a href="#检索阶段：获取相关证据" class="headerlink" title="检索阶段：获取相关证据"></a><strong>检索阶段：获取相关证据</strong></h4><p><strong>输入</strong>：用户查询（如“2025年华为云新产品发布会时间”）。</p>
<p><strong>处理</strong>：</p>
<ol>
<li><p><strong>查询向量化</strong>：使用与索引阶段相同的嵌入模型，将用户查询转换为向量；</p>
</li>
<li><p><strong>相似度检索</strong>：在向量数据库中查找与查询向量“距离”最近的K个文档片段（如K&#x3D;3），常用相似度 metric 包括余弦相似度、欧氏距离；</p>
</li>
<li><p><strong>结果过滤</strong>：去除低相关度结果（如相似度低于阈值0.7），保留最相关的证据。</p>
</li>
</ol>
</li>
<li><h4 id="增强阶段：构造增强提示"><a href="#增强阶段：构造增强提示" class="headerlink" title="增强阶段：构造增强提示"></a><strong>增强阶段：构造增强提示</strong></h4><p><strong>输入</strong>：用户查询、检索到的证据片段。</p>
<p><strong>处理</strong>：</p>
<ol>
<li><p><strong>提示工程</strong>：将证据与查询整合为结构化的提示（Prompt），例如：</p>
<p> “请根据以下信息回答问题：</p>
<p>​	信息1：[2025年华为云新产品发布会将于3月15日在深圳举行]</p>
<p>​	信息2：[发布会将推出新一代GPU服务器]</p>
<p>​	问题：2025年华为云新产品发布会的时间是什么时候？”</p>
</li>
<li><p><strong>目标</strong>：引导LLM聚焦于证据，避免“脱离上下文”的生成。</p>
</li>
</ol>
</li>
<li><h4 id="生成阶段：输出准确答案"><a href="#生成阶段：输出准确答案" class="headerlink" title="生成阶段：输出准确答案"></a><strong>生成阶段：输出准确答案</strong></h4><p><strong>输入</strong>：增强后的提示。</p>
<p><strong>处理</strong>：</p>
<ol>
<li><p><strong>LLM</strong>基于提示中的证据，生成符合要求的答案；</p>
</li>
<li><p><strong>输出要求</strong>：答案需<strong>基于证据</strong>（如引用信息1中的时间）、<strong>简洁明了</strong>（如直接回答“3月15日”）、<strong>可追溯</strong>（如标注信息来源）。</p>
</li>
</ol>
</li>
</ol>
<p>相较于传统LLM，<strong>RAG的优势</strong>主要体现在以下四个方面：</p>
<ol>
<li><h4 id="减少幻觉，提升准确性"><a href="#减少幻觉，提升准确性" class="headerlink" title="减少幻觉，提升准确性"></a><strong>减少幻觉，提升准确性</strong></h4><p>LLM的“幻觉”源于其“参数化记忆”（训练数据中的噪声或知识冲突），而RAG通过<strong>外部证据</strong>约束生成过程，使回答更具事实依据。例如，在医疗领域，RAG可基于最新的临床指南回答患者问题，避免因训练数据过时导致的错误建议。</p>
</li>
<li><h4 id="实时更新知识，突破“知识天花板”"><a href="#实时更新知识，突破“知识天花板”" class="headerlink" title="实时更新知识，突破“知识天花板”"></a><strong>实时更新知识，突破“知识天花板”</strong></h4><p>LLM的训练数据是静态的（如GPT-4的知识截止到2023年10月），无法获取2023年之后的最新信息（如2025年的政策、事件）。RAG通过<strong>动态更新外部知识库</strong>（如定期同步最新文档、网页），使模型能回答“2025年的新问题”，突破模型参数的“知识天花板”。</p>
</li>
<li><h4 id="增强可追溯性，提升用户信任"><a href="#增强可追溯性，提升用户信任" class="headerlink" title="增强可追溯性，提升用户信任"></a><strong>增强可追溯性，提升用户信任</strong></h4><p>RAG的答案<strong>标注了信息来源</strong>（如“根据2025年华为云官方公告”），用户可通过来源核实答案的准确性。这种“透明性”大幅提升了用户对AI系统的信任，尤其在金融、医疗等高风险领域具有重要意义。</p>
</li>
<li><h4 id="降低成本，提高效率"><a href="#降低成本，提高效率" class="headerlink" title="降低成本，提高效率"></a><strong>降低成本，提高效率</strong></h4><p>相较于“微调”（重新训练LLM以适应特定领域），RAG的成本更低、效率更高：<strong>无需重新训练</strong>：只需更新外部知识库，即可让模型掌握新领域知识；<strong>快速迭代</strong>：知识库的更新可在数小时内完成，而微调需数天甚至数周；<strong>资源消耗少</strong>：向量数据库的存储与检索成本远低于LLM的训练成本。</p>
</li>
</ol>
<p>说了这么多优点，那<strong>缺点</strong>呢？RAG目前的关键难点是：<strong>检索质量决定一切</strong>、<strong>上下文窗口限制</strong>、<strong>“语义漂移”</strong>、<strong>模型仍可能编造</strong>。如果检索错了，生成就会基于错误内容。因为RAG系统会将长文档进行分块（取决于切片的算法，但目前都不成熟），可能会导致语句被拦腰截断，AI拿到被拆碎的段落，很难理解上下文的关系，所以输出的回答也不会太精准，现有解决方案是可以在检索数据并在向量数据库匹配后，再<strong>使用重排序模型进行更深入的语义分析</strong>，然后再按照问题的相关性，进行重新的排序，把相关性最大的一些数据排到前面，然后交付给LLM，这是一种先粗后细的两步检索策略，可以进一步提高检索精度。</p>
<p><strong>LLM是生成器，RAG是知识桥梁，RAG本质是把静态模型变成动态知识系统。</strong>真正难的不是生成，而是“找对信息”。</p>
<h2 id="MCP"><a href="#MCP" class="headerlink" title="MCP"></a>MCP</h2><p>　　MCP是<strong>Model Context Protocol</strong>，中文叫“模型上下文协议”，是由Anthropic于2024年底推出的<strong>开放标准协议</strong>，旨在解决大语言模型（LLM）与外部数据源、工具集成的“碎片化”难题。它被业界形象地称为“<strong>AI的USB-C接口</strong>”——通过标准化、双向的通信通道，连接LLM与外部服务（如数据库、API、文件系统等），实现“即插即用”的能力扩展。</p>
<p>MCP的核心理念是<strong>“标准化交互”</strong>：将LLM与外部工具的连接从“定制开发”转变为“<strong>通用协议适配</strong>”，让不同模型（如Claude、GPT-4、Llama）与不同工具（如GitHub、Google Drive、企业内部系统）通过MCP这一“共同语言”无缝协作，彻底解决“M×N”集成问题（即M个模型对接N个工具需要M×N种适配）。</p>
<p>LLM本身世封闭的，它不能主动访问数据库、不能主动读文件、不能主动调用 API，除非你把数据拼进 prompt。如果把每个工具都用自定义方式接入，系统会变得非常混乱。<strong>MCP 的目标是：让工具接入变成标准化。</strong></p>
<p>MCP是AI领域的“万能接口”，是工具层的标准化基石。它通过标准化协议，让LLM能够安全、高效地连接外部工具与数据，彻底解决了“碎片化”集成问题，<strong>MCP的出现，标志着AI从“孤岛式智能”向“连接式智能”的转变，它将让AI真正成为“能解决实际问题”的工具。</strong></p>
<h2 id="Skills"><a href="#Skills" class="headerlink" title="Skills"></a>Skills</h2><p>　　“Skills” 在 LLM 生态里不是一个严格统一的学术术语，而是一个工程概念。它通常指：<strong>把某种可复用的能力，打包成可被模型调用或触发的模块。</strong>一个 Skill 通常包含三部分：<strong>触发规则</strong>（什么时候用它）、<strong>执行逻辑</strong>（它做什么）、<strong>输出格式</strong>（结果如何返回）。Skill是”能力打包“，把Prompt 工程产品化，它把一次性的提示词，变成：可版本管理、可共享、可升级、可标准化。</p>
<p>为什么需要Skill？</p>
<p>因为模型本身是“概率生成器”。没有结构约束时：风格不稳定、输出不可预测、易偏离目标。Skill 提供：模板、边界、规则、可复用性。这<strong>让模型从“灵感机器”变成“生产工具”。</strong>Skill对模型能力的结构化封装，使其成为可复用、可预测、可组合的能力模块。<strong>Skills是连接LLM与实际应用的核心桥梁，是实现“从对话到行动”的关键能力封装。它并非简单的工具调用，而是将专业领域的方法论、执行流程、工具资源打包成可复用、可组合的标准化模块</strong>，让AI智能体（Agent）具备特定领域的专业能力，如“清理电脑垃圾文件”、“生成工业级设计稿”、“分析股票数据”等。</p>
<h2 id="AI-Agent"><a href="#AI-Agent" class="headerlink" title="AI Agent"></a>AI Agent</h2><p>　　AI Agent其实很简单，就是上述提到的所有次的集合体，LLM本来就能思考和规划，给它加上了Memory就能让它记住历史，加上RAG让它能获取外部知识，加上MCP和Skills让它能操作工具，它们共同构成了一个在某些功能上能代替人类自主行动完成目标的一个AI系统，这个系统就叫AI Agent。</p>
<p>说了这么多，我们也就能明白一件事情，那就是所有这些归根究底就是Prompt的各种变体，也就是提示词工程，之所以需要如此多的规范和约定，就是因为LLM的本质是一个语言概率模型，它就是无法做到像程序一样100%可控，需要靠一大堆非常严格的Prompt来约定LLM的输出。所以今天所有的Agent技术，离真正自动化系统还有明显的距离，本质上仍然实在语言之上硬塞各种操作能力，想要变得更稳更可控，还是只能在模型工具的调度层上继续演进和优化。</p>
<p>　　可以把Agent看成是所有不需要智能的地方构成的部分，一个流程当中所有能用固定的程序来解决而不需要问LLM的地方就是Agent发挥作用的地方。其实就是把模糊的分流逻辑交给大模型，根据语义识别出用户想做a还是b，把确定的分流逻辑交给程序，比如说PDF提取文本，最终的目标都是节省人类的时间，降低人类的使用门槛。现在没办法能真正铺开的原因是因为现在Token实在太太太贵了，越是强大的自己能默默处理问题的Agent，背后消耗的Token就越大。</p>
<p>另外我想说下为什么我觉得现在的这些提示词工程都只是一个中间态，参考Java领域的SpringBoot和Python领域的UV，你会发现这两者都是将开发者的便利完全放在第一位，什么运行速度快不快，包的体积大不大，是不是浪费内存空间或磁盘空间，这些问题最终和使用的便利性相比，几乎都是瞬间被秒杀了，LLM时代就是典型的空间换时间的例子，用海量的参数换取更少的在线计算步骤。</p>
<p>最后，<strong>Clawdbot</strong>为什么突然爆火了？除了一些营销因素以外，它其实和早已经出现的ClaudeCode、Codex、Manus这些Agent并没有本质的区别，只是因为他能连接社交软件，能够配置定时任务，有UI页面能看到Skill并管理它们，第一次让普通人觉得它像一个智能体，而不只是躺在电脑上的一个服务了。</p>
<hr>
<p>　　最后的最后，我想简单聊聊在AI这个发展速度以小时记的时代，我们改怎么获取与处理信息源？在信息爆炸，碎片化信息充斥互联网的时代，你的信息来源、加工信息的方法以及产出的成果，决定了它能不能成为成长的燃料，否则就是内耗、磨损和成本。但在聊信息源之前，我们需要先想清楚一件事，那就是在AI时代，你想扮演什么样的人？你想要什么程度的参与，就决定了你需要什么样的信息，比如我，我的想法其实很简单，就是通过科技，智能化生活，提高日常生活的幸福度，比如HomeAssistant、OpenWrt、PVE等等，我的信息来源分三个层次，分别是News、Source和Content。当你需要最近的新玩意他是什么东西的时候，你就需要News；而所有新鲜事物、新技术、新方向最终落地的地方就叫Source，是你能第一时间看得到，摸得到它完整版的地方。对我来说基本就是Github，偶尔会看看ProductHunt；最后Content，它是一种经过二次加工或者更多次加工的信息，它是为理解事物的效率而服务的，我们需要内容，但也需要知道它也是造成焦虑、内耗和FOMO(Fear Of Missing Out担心踏空)的最重要的原因，我们要知道这个世界的内容都是受算法支配的，但凡是博主，他们都希望内容被更多的人看到，他们就必须要迎合算法，要符合大众，要有情绪价值，因此很多内容就要么越来越肤浅，要么更倾向于浮夸，更倾向于热点，这次Clawdbot的爆火，就是个很好的例子，网上的大部分内容其实都很浮夸，这些内容基本也就看个热闹，对成长帮助并不大，所以在看内容的时候，我们要知道内容的背后一定有一个被算法支配的人。所以我们要找到帮我们理解事物的信息，但必须懂得筛选，内容要符合你的定位，要能帮你理解你的Source，要能帮你快速获取News，而且要警惕那些不理性的兴奋或者是沮丧。</p>
<p>2011年出版的一本书《Information Diet》（信息节食），作者是Clay Johnson，背景是社交媒体爆炸式增长、政治极化加剧、注意力经济成型。那时的推特还很年轻，Facebook 还没被贴上“老年人社区”的标签。Johnson 看到的是一种新型公共健康问题：信息肥胖。Clay Johnson提出过一种概念，就是我们今天对于信息的摄入，就如同是暴饮暴食，产生了信息肥胖，以及一系列的认知疾病，比如认知能力退化、注意力涣散、决策质量下降，也就是FOMO的情绪根源。他认为，我们应该进行“信息节食”，区分信息源；进行注意力健身，也就是有意识的增加深度专注时间；改变消费策略，改被动饮食为主动寻找健康食品。并不是信息过载了，而是信息消费不当，就如同肥胖一样，食物很多，不一定都吃，都吃不是食物的问题，而是我们摄入策略有问题了。要建立信息食谱，荤素搭配，按照合理比例分配深度和浅度的信息摄入比例，营养均衡，少吃零食（社交媒体上的焦虑体）。这是人类历史上第一次，我们面临的不是“如何获取信息”，而是“如何拒绝信息”。这本书只是开了个头。真正的难题，是在一个自动生成无限文本的时代，如何维持判断力的肌肉张力。</p>
<p>FOMO的反面是JOMO（Joy Of Missing Out接受错过），成熟决策往往意味着：主动放弃 90% 的机会。真正的能力不是抓住所有机会，而是判断：哪 10% 值得投入。在AI时代FOMO表现的特别明显：LLM、Agent、RAG、MCP、新框架每天一个，很多人会产生：“我不全学是不是就淘汰？”但现实是：底层原理变化很慢，表层名词变化很快。而FOMO 常发生在表层。经常和我聊天一个“霸总”哥们，我就能感觉到他的FOMO，当然FOMO并不完全是坏事，进化不是平稳增长，而是压力驱动的适应，FOMO的问题并不是错过机会，而是在不确定情况下，过度高估错过的损失，他会让人冲动决策、高位接盘、盲目跟风、过度分散注意力，牛马每天上个班就已经很累了，上班领导压力你，下班就不要再自己压力自己了，FOMO 是对“被时代抛下”的焦虑，而不是对机会本身的评估。真正的成长往往来自：深耕，而不是追逐。</p>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源，欢迎对文章中的引用来源进行考证，欢迎指出任何有错误或不够清晰的表达。可以在下面评论区评论，也可以邮件至 mengziforever@qq.com </span>
    </div>
</article>





    <div id="comments"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">

<script type="text/javascript">
    $.getScript('/js/gitalk.js', function () {
        var gitalk = new Gitalk({
            clientID: 'bd05684c0f17859f0666',
            clientSecret: 'e5b31b0a057232e2ec744b6dfadab98fe07dfe15',
            repo: 'rosemengzi.github.io',
            owner: 'rosemengzi',
            admin: ['rosemengzi'],
            id: decodeURI(location.pathname),
            distractionFreeMode: 'true',
            language: 'zh-CN',
            perPage: parseInt('10',10)
        })
        gitalk.render('comments')
    })
</script>




    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2022-2024 mengzi
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

</body>
<script src="/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 542px;
    }
    .nav.fullscreen {
        margin-left: -542px;
    }
    .nav-left {
        width: 120px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 492px;
        }
        .nav.fullscreen {
            margin-left: -492px;
        }
        .nav-left {
            width: 100px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 492px;
            margin-left: -492px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    

    
</style>







</html>
